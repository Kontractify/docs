---
title: AI Data Quality Analysis
description: "Evaluate the quality of your survey responses with AI"
icon: "badge-check"
---

<Tip>
  Looking for the [API reference](/api-reference/data-quality/evaluate)?
</Tip>

## Introduction

Aftercare evaluates the quality of your responses in the context of the response itself and compared to all other survey responses. Not all
quality checks are built the same, so we've built a few different metrics to give you a holistic view of your responses.

## Components

Aftercare AI breaks down the quality of your responses into a few different metrics:

- `Nonsense`: how coherent and logical the response itself is. (ie, gibberish, troll responses, etc.)
- `Relevance`: how pertinent the response is to the question.
- `Low-effort`: how much effort was put into the response. (length, specific details, etc.)
- `Llm-generated`: how likely the response is to be AI or LLM generated.
- `Self-duplication`: how similar a particular answer is to previous answers in the same survey response.
- `Shared-duplication`: how similar the response is to other responses for a particular question across respondents

Aftercare will generate a confidence score for each of these metrics along with an overall quality score you can use to compare the quality of your responses.
Aftercare will let you know which (if any) of these quality metrics were violated and how many violations were found in total.

## Quality Detection Modes

In addition to checking for specific quality issues, Aftercare provides three detection modes that group related issues together for common use cases:

<Card title="Responsiveness" icon="reply">
  Focuses on identifying poor-quality responses by checking for issues like
  nonsensical content, irrelevant answers, and low-effort responses.
  <Tip>
    Use this mode when you want to quickly identify respondents who aren't
    engaging meaningfully with your survey.
  </Tip>
</Card>

<Card title="Authenticity" icon="shield-check">
  Concentrates on detecting potentially fraudulent responses by checking for
  issues like LLM-generated content and duplicated answers across respondents.
  <Tip>
    Use this mode when validating that responses are genuine and not
    artificially generated.
  </Tip>
</Card>

<Card title="Composite" icon="list-check">
  Performs a comprehensive evaluation by checking for all available quality
  issues. This is the most thorough analysis and is the default mode.
  <Tip>
    Use this mode when you want a comprehensive check of all quality factors
    across your dataset.
  </Tip>
</Card>

<Note>
  If you specify both a detection mode and a list of specific quality issues in
  your request, Aftercare will prioritize the detection mode.
</Note>

## Tips for improving your response quality

While you can use the data quality API to evaluate each response individually, providing a `survey ID`, `question ID`, and `response ID` will help tie together responses across respondents.
Doing so will provide a more accurate assessment of the quality of your responses.
